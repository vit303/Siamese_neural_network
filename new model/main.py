import tensorflow as tf
from keras import layers, models
from keras import backend as K
import numpy as np
import matplotlib.pyplot as plt
import random
import os
import cv2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import re

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # –†–∞–∑—Ä–µ—à–∞–µ–º —Ä–æ—Å—Ç –ø–∞–º—è—Ç–∏ GPU
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        strategy = tf.distribute.MirroredStrategy()
        print(f'Number of devices: {strategy.num_replicas_in_sync}')
    except RuntimeError as e:
        print(e)
else:
    strategy = tf.distribute.get_strategy()
    print("GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

# 1. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
def create_base_network(input_shape):
    """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤—É—é —Å–µ—Ç—å-—ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è –¥–µ—Ñ–µ–∫—Ç–æ–≤ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏"""
    with strategy.scope():
        model = models.Sequential([
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
            layers.MaxPooling2D((2, 2)),
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Conv2D(128, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.Conv2D(256, (3, 3), activation='relu'),
            layers.GlobalAveragePooling2D(),
            layers.Dense(512, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(128, activation='relu')  # –í—ã—Ö–æ–¥–Ω–æ–µ embedding-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
        ])
    return model

def euclidean_distance(vectors):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–æ–≤"""
    vector1, vector2 = vectors
    sum_square = tf.reduce_sum(tf.square(vector1 - vector2), axis=1, keepdims=True)
    return tf.sqrt(tf.maximum(sum_square, tf.keras.backend.epsilon()))

def contrastive_loss(y_true, y_pred, margin=1.0):
    """–ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å"""
    y_true = tf.cast(y_true, tf.float32)
    square_pred = tf.square(y_pred)
    margin_square = tf.square(tf.maximum(margin - y_pred, 0))
    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)

def create_siamese_network(input_shape):
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—É—é —Å–∏–∞–º—Å–∫—É—é —Å–µ—Ç—å"""
    with strategy.scope():
        # –î–≤–∞ –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        input_a = layers.Input(shape=input_shape)
        input_b = layers.Input(shape=input_shape)
        
        # –ë–∞–∑–æ–≤–∞—è —Å–µ—Ç—å (–æ–±—â–∏–µ –≤–µ—Å–∞)
        base_network = create_base_network(input_shape)
        
        # –í—ã—Ö–æ–¥—ã –¥–ª—è –¥–≤—É—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        processed_a = base_network(input_a)
        processed_b = base_network(input_b)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É embedding'–∞–º–∏
        distance = layers.Lambda(euclidean_distance, output_shape=(1,))([processed_a, processed_b])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model = models.Model([input_a, input_b], distance)
    return model

# 2. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö NEU Surface Defect Dataset
def extract_defect_class(filename):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª–∞—Å—Å –¥–µ—Ñ–µ–∫—Ç–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
    # –ü—Ä–∏–º–µ—Ä: crazing_1.jpg.rf.06382e752d1e00ca... -> crazing
    match = re.match(r'^([a-zA-Z]+)_', filename)
    if match:
        return match.group(1).lower()
    return "unknown"

def load_neu_surface_defect_data(data_path='model/train'):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö NEU Surface Defect Dataset"""
    images = []
    labels = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—É—Ç–∏
    if not os.path.exists(data_path):
        # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø—É—Ç–∏
        alternative_paths = [
            'model/train',
            '../model/train',
            './model/train',
            'train',
            '../train'
        ]
        
        for alt_path in alternative_paths:
            if os.path.exists(alt_path):
                data_path = alt_path
                print(f"–ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏: {data_path}")
                break
        else:
            # –ï—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω –ø—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–æ–∫–∞–∂–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞–ø–∫–∏
            current_dir = os.getcwd()
            print(f"–¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {current_dir}")
            print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞–ø–∫–∏:")
            for item in os.listdir(current_dir):
                if os.path.isdir(item):
                    print(f"  - {item}")
            raise ValueError(f"–ü–∞–ø–∫–∞ {data_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
    
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {data_path}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_files = [f for f in os.listdir(data_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
    
    if not image_files:
        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ!")
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    for img_file in image_files:
        img_path = os.path.join(data_path, img_file)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª–∞—Å—Å –¥–µ—Ñ–µ–∫—Ç–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        defect_class = extract_defect_class(img_file)
        
        try:
            # –ß—Ç–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img = cv2.imread(img_path)
            if img is None:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {img_file}")
                continue
                
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (200, 200))  # –†–∞–∑–º–µ—Ä –¥–ª—è NEU dataset
            img = img.astype('float32') / 255.0
            
            images.append(img)
            labels.append(defect_class)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {img_file}: {e}")
            continue
    
    print(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"–ù–∞–π–¥–µ–Ω—ã –∫–ª–∞—Å—Å—ã: {set(labels)}")
    
    return np.array(images), np.array(labels)

def create_defect_pairs(x, y, num_pairs_per_class=300):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–µ—Ñ–µ–∫—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    unique_classes = np.unique(y)
    print(f"–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä –¥–ª—è –∫–ª–∞—Å—Å–æ–≤: {unique_classes}")
    
    class_indices = {cls: np.where(y == cls)[0] for cls in unique_classes}
    
    pairs = []
    labels = []
    
    for cls in unique_classes:
        indices = class_indices[cls]
        print(f"–ö–ª–∞—Å—Å {cls}: {len(indices)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        # Positive pairs - –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –¥–µ—Ñ–µ–∫—Ç–æ–≤
        if len(indices) >= 2:
            for i in range(min(num_pairs_per_class, len(indices) * 2)):
                try:
                    idx1, idx2 = random.sample(list(indices), 2)
                    pairs.append([x[idx1], x[idx2]])
                    labels.append(1)  # 1 - –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –¥–µ—Ñ–µ–∫—Ç—ã
                except ValueError:
                    break
        
        # Negative pairs - —Ä–∞–∑–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–µ—Ñ–µ–∫—Ç–æ–≤
        other_classes = [c for c in unique_classes if c != cls]
        for i in range(num_pairs_per_class):
            if other_classes and indices.size > 0:
                other_cls = random.choice(other_classes)
                other_indices = class_indices[other_cls]
                if other_indices.size > 0:
                    idx1 = random.choice(indices)
                    idx2 = random.choice(other_indices)
                    pairs.append([x[idx1], x[idx2]])
                    labels.append(0)  # 0 - —Ä–∞–∑–Ω—ã–µ –¥–µ—Ñ–µ–∫—Ç—ã
    
    pairs = np.array(pairs)
    labels = np.array(labels)
    
    print(f"–°–æ–∑–¥–∞–Ω–æ {len(pairs)} –ø–∞—Ä: {np.sum(labels==1)} –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö, {np.sum(labels==0)} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö")
    
    return pairs, labels

# 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def train_siamese_network():
    """–ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å–∏–∞–º—Å–∫–æ–π —Å–µ—Ç–∏ –¥–ª—è –¥–µ—Ñ–µ–∫—Ç–æ–≤ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏"""
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö NEU Surface Defect Dataset...")
    x_data, y_data = load_neu_surface_defect_data('model/train')
    
    if len(x_data) == 0:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å –∫ dataset.")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    x_train, x_test, y_train, y_test = train_test_split(
        x_data, y_data, test_size=0.2, random_state=42, stratify=y_data
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä
    print("–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –ø–∞—Ä...")
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    pairs_per_class = min(200, len(x_train) // len(np.unique(y_train)) // 2)
    train_pairs, train_labels = create_defect_pairs(x_train, y_train, num_pairs_per_class=pairs_per_class)
    test_pairs, test_labels = create_defect_pairs(x_test, y_test, num_pairs_per_class=pairs_per_class // 2)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –¥–≤–∞ –º–∞—Å—Å–∏–≤–∞
    x_train_1 = train_pairs[:, 0]
    x_train_2 = train_pairs[:, 1]
    x_test_1 = test_pairs[:, 0]
    x_test_2 = test_pairs[:, 1]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–∞–º—Å–∫–æ–π —Å–µ—Ç–∏...")
    input_shape = (200, 200, 3)  # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π NEU dataset
    
    with strategy.scope():
        siamese_net = create_siamese_network(input_shape)
        siamese_net.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), 
            loss=contrastive_loss, 
            metrics=['accuracy']
        )
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    batch_size = 32 * strategy.num_replicas_in_sync
    epochs = 50  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è batch size: {batch_size}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫: {strategy.num_replicas_in_sync}")
    
    # Callbacks
    callbacks = [
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7),
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        tf.keras.callbacks.ModelCheckpoint(
            'best_siamese_defect_model.h5',
            monitor='val_loss',
            save_best_only=True,
            mode='min'
        )
    ]
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    history = siamese_net.fit(
        [x_train_1, x_train_2], train_labels,
        validation_data=([x_test_1, x_test_2], test_labels),
        batch_size=batch_size,
        epochs=epochs,
        verbose=1,
        callbacks=callbacks
    )
    
    return siamese_net, history, (x_test, y_test)

# 4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
def plot_training_history(history):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('training_history.png')
    plt.show()

# 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def save_model(model, history):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ–π –º–æ–¥–µ–ª–∏
    model.save('siamese_surface_defect_model.h5')
    print("–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'siamese_surface_defect_model.h5'")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    np.save('surface_defect_training_history.npy', history.history)
    print("–ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'surface_defect_training_history.npy'")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π —Å–µ—Ç–∏
    base_network = model.layers[2]  # –ë–∞–∑–æ–≤–∞—è —Å–µ—Ç—å
    base_network.save('surface_defect_base_network.h5')
    print("–ë–∞–∑–æ–≤–∞—è —Å–µ—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'surface_defect_base_network.h5'")

# 6. –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def test_model(model, x_test, y_test, num_tests=5):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    for i in range(num_tests):
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        idx1 = random.randint(0, len(x_test) - 1)
        idx2 = random.randint(0, len(x_test) - 1)
        
        img1 = x_test[idx1]
        img2 = x_test[idx2]
        label1 = y_test[idx1]
        label2 = y_test[idx2]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        distance = model.predict([np.expand_dims(img1, 0), np.expand_dims(img2, 0)], verbose=0)[0][0]
        similarity = 1 - min(distance, 1.0)
        
        print(f"–¢–µ—Å—Ç {i+1}:")
        print(f"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 1: –¥–µ—Ñ–µ–∫—Ç {label1}")
        print(f"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 2: –¥–µ—Ñ–µ–∫—Ç {label2}")
        print(f"  –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance:.3f}")
        print(f"  –°—Ö–æ–∂–µ—Å—Ç—å: {similarity:.3f}")
        print(f"  –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {'–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ' if label1 == label2 else '—Ä–∞–∑–Ω—ã–µ'}")
        print()

# 7. –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def main():
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞"""
    print("=== –û–±—É—á–µ–Ω–∏–µ —Å–∏–∞–º—Å–∫–æ–π —Å–µ—Ç–∏ –Ω–∞ NEU Surface Defect Dataset ===")
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {type(strategy).__name__}")
    print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ GPU: {len(gpus) if gpus else 0}")
    
    # –ü–æ–∫–∞–∂–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    current_dir = os.getcwd()
    print(f"–¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {current_dir}")
    print("–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:")
    for item in os.listdir(current_dir):
        item_path = os.path.join(current_dir, item)
        if os.path.isdir(item_path):
            print(f"  üìÅ {item}")
        else:
            print(f"  üìÑ {item}")
    
    try:
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model, history, (x_test, y_test) = train_siamese_network()
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        plot_training_history(history)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        save_model(model, history)
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        test_model(model, x_test, y_test, num_tests=8)
        
        print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        print("–ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–µ—Ñ–µ–∫—Ç–æ–≤ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏!")
        
    except Exception as e:
        print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö")
        import traceback
        traceback.print_exc()

# –ó–∞–ø—É—Å–∫
if __name__ == "__main__":
    main()